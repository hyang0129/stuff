{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf \nimport numpy as np \n\nprint(tf.__version__)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2.1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\ndef get_layers(layer): \n    try: \n        return layer.layers\n    except AttributeError: \n        return []\n\ndef get_mult(layer):\n    #if not mult then assume 1\n    try:\n        return layer.lr_mult\n    except AttributeError:\n        return 1.\n    \ndef assign_mult(layer, lr_mult):\n    #if has mult, don't override\n    try:\n        layer.lr_mult \n    except AttributeError: \n        layer.lr_mult = lr_mult \n    \ndef get_lowest_layers(model):\n    layers = get_layers(model)\n    \n    mult = get_mult(model)\n    \n    if len(layers) > 0: \n        for layer in layers: \n            #propage mult to lower layers\n            assign_mult(layer, mult)\n            for sublayer in get_lowest_layers(layer):\n                yield sublayer\n    else:\n        yield model\n    \ndef apply_mult_to_var(layer): \n    mult = get_mult(layer)\n    for var in layer.trainable_variables:\n        var.lr_mult = tf.convert_to_tensor(mult, tf.float32)\n\n    return layer\n\ndef inject(model): \n    \n    for layer in get_lowest_layers(model): \n        apply_mult_to_var(layer) \n    \n    #get opt, move the original apply fn to a safe place, assign new apply fn \n    opt = model.optimizer\n    opt._apply_gradients = opt.apply_gradients\n    opt.apply_gradients = apply_gradients.__get__(opt)\n    opt.testing_flag = True \n    \n    return model\n    \ndef apply_gradients(self, grads_and_vars, *args, **kwargs): \n    \n    if self.testing_flag: \n        print('Training with layerwise learning rates')\n        self.testing_flag = False\n        \n    grads = [] \n    var_list = [] \n    \n    #scale each grad based on var's lr_mult\n    for grad, var in grads_and_vars:\n        grad = tf.math.scalar_mul(var.lr_mult, grad)\n        grads.append(grad)\n        var_list.append(var)\n    \n    grads_and_vars = list(zip(grads, var_list))\n        \n    return self._apply_gradients(grads_and_vars, *args, **kwargs)\n\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\ndef build_simple_model(opt, loss = 'binary_crossentropy'): \n\n    sub_model = tf.keras.Sequential([tf.keras.layers.Dense(5, activation=tf.nn.relu)\n                                     , tf.keras.layers.Dense(5, activation=tf.nn.relu)])\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(1,)),  \n        sub_model,        \n        tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n    ])\n\n    model.compile(loss = loss, optimizer = opt)\n    return model \n\ndef test_lr_mult(model, do_inject = True):\n    if do_inject: \n        inject(model)\n    x = np.ones(shape = (256,1), dtype = np.float32)\n    y = np.ones(shape = (256,1), dtype = np.float32)\n    return model.fit(x, y, batch_size = 32, epochs = 5, verbose = 0)\n\n\ndef h_to_list(h):\n    return h.__dict__['history']['loss']\n\ndef test_zero_lr_mult(model_fn = build_simple_model\n                      , opts = ['adam', 'sgd']\n                      , losses = ['binary_crossentropy', 'MSE']): \n    #test highest level \n    \n    for opt in opts: \n        for loss in losses: \n\n            model = model_fn(opt, loss)\n            model.lr_mult = 0\n            h = test_lr_mult(model)\n            assert len(set(h.__dict__['history']['loss'])) == 1, 'WITH 0 LR ALL LOSSES SHOULD BE IDENTICAL'\n\n            #test top level layer\n            model = model_fn(opt, loss)\n            for layer in model.layers: \n                layer.lr_mult = 0 \n            h = test_lr_mult(model)\n            assert len(set(h.__dict__['history']['loss'])) == 1, 'WITH 0 LR ALL LOSSES SHOULD BE IDENTICAL'\n            \n\ndef test_some_lr_mult(model_fn = build_simple_model\n                      , opts = ['adam', 'sgd']\n                      , losses = ['binary_crossentropy', 'MSE']): \n    \n    for opt in opts: \n        for loss in losses: \n            \n            model = model_fn(opt, loss)\n            h = test_lr_mult(model)\n            h = h_to_list(h)\n            assert h[0] > h[-1], 'LOSS SHOULD HAVE DECREASED'\n\n            model = model_fn(opt, loss)\n            model.lr_mult = 1\n            h = test_lr_mult(model)\n            h = h_to_list(h)\n            assert h[0] > h[-1], 'LOSS SHOULD HAVE DECREASED'\n            \n            model = model_fn(opt, loss)\n            model.layers[0].lr_mult = 0\n            h = test_lr_mult(model)\n            h = h_to_list(h)\n            assert h[0] > h[-1], 'LOSS SHOULD HAVE DECREASED'\n            \n    \ntest_zero_lr_mult(build_simple_model)\ntest_some_lr_mult(build_simple_model)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\nTraining with layerwise learning rates\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}